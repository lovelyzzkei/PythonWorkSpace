# -*- coding: utf-8 -*-
"""Xception+CenterCrop

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q7s4xih6ywwGbf6Hoy2xkNvkgNefT686
"""

# from google.colab import drive
# drive.mount('/content/gdrive/')

import os
import cv2
import random
import numpy as np
import pandas as pd
from glob import glob
from sklearn.utils import shuffle
import tensorflow as tf

fake_paths=glob('/D:/deepfake_1st/fake/*/*/*/*/*/*.jpg')
real_paths=glob('/D:/deepfake_1st/fake/*/*/*/*/*/*.jp')

LABEL=[]
PATH=[]

all_paths = fake_paths + real_paths

random.shuffle(all_paths)

labeling={'fake':0, 'real':1}
for path in all_paths:
  LABEL.append(labeling[path.split('/')[6]])
  PATH.append(path)

train_df=pd.DataFrame(PATH, LABEL).reset_index()
train_df.columns=['label', 'path']
#validation_df = train_df[0:3000]
train_df = train_df[:]

AUTO = tf.data.experimental.AUTOTUNE



#데이터 셋을 만들어주는 함수
def create_dataset(df, training, batch_size, input_size):
  #경로를 통해서 이미지를 변환시켜주는 함수
  def read_image(image_path, label):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = (tf.cast(image, tf.float32) / 127.5 ) - 1
    return image, label
  
  image_paths, labels = df.path, df.label
  
  #from_tensor_slices를 통해서 대용량 데이터를 train data로 구축해줍니다.
  dataset=tf.data.Dataset.from_tensor_slices((image_paths, labels))
  if training:
    dataset=dataset.shuffle(2048)
    
  dataset=dataset.map(read_image, num_parallel_calls=AUTO)
    
  dataset=dataset.batch(batch_size)
  dataset = dataset.prefetch(AUTO)

  return dataset

train_ds=create_dataset(
        df=train_df,
        training=True, 
        batch_size=6,   # 이거 batch_size 안줄이면 GPU_Memory 넘어가서 오류발생 
        input_size=(540, 960, 3)
)

# val_ds = create_dataset(
#     df=validation_df,
#     training=True,
#     batch_size=6,
#     input_size=(540, 960, 3)
# )

import tensorflow as tf
from tensorflow import keras

def model():
  pretrained_model = keras.applications.xception.Xception(
    include_top=False,
    weights='imagenet',
    input_shape=[299, 299, 3],
  )
  pretrained_model.trainable = True

  inputs = keras.layers.Input(shape=(540, 960, 3))
  x = keras.layers.CenterCrop(299, 299)(inputs)
  x = pretrained_model(x)
  x = keras.layers.GlobalAveragePooling2D()(x)
  x = keras.layers.Dense(100, activation='relu')(x)
  output = keras.layers.Dense(1, activation='sigmoid')(x)

  model = keras.models.Model(inputs = inputs, outputs = output)

  model.compile(
    optimizer = keras.optimizers.Adam(learning_rate=0.001), 
    loss = keras.losses.BinaryCrossentropy(from_logits=False), 
    metrics = ['accuracy']
  )

  return model

model = model()

from keras import callbacks

filepath = '/D:/weights.hdf5'

mcp = callbacks.ModelCheckpoint(filepath, monitor = 'loss', save_best_only=True, save_weights_only=True, mode='min', save_freq=1000)
es = callbacks.EarlyStopping(monitor='accuracy', min_delta=0.01, patience=10, mode='max', restore_best_weights=False)

#STEPS_PER_EPOCH = 30
epochs = 1

model.fit(train_ds, epochs= epochs, verbose=1, callbacks=[mcp])

model.save('/D:/test_model/')

model = tf.keras.models.load_model('/D:/test_model/')
dir = '/D:/submission.csv'
sc = open(dir, 'w')
sc.write('path,y')
sc.write('\n')

test_path = '/D:/test/leaderboard/leaderboard/*.jpg'

def read_image(image_path):
  image = tf.io.read_file(image_path)
  image = tf.image.decode_jpeg(image, channels=3)
  image = (tf.cast(image, tf.float32) / 127.5 ) - 1
  return image

# collect all images
images = glob(test_path)
images.sort()

test_set=tf.data.Dataset.from_tensor_slices(images)
test_set=test_set.map(read_image)
test_set = test_set.batch(1)

predictions = model.predict(test_set)

predictions[:100]

for i in range(len(images)):
  if predictions[i] >= 0.7:
    saveline = images[i][-27:] + ',1'
    sc.write(saveline)
    sc.write('\n')
  else:
    saveline = images[i][-27:] + ',0'
    sc.write(saveline)
    sc.write('\n')

sc.close()

submission = pd.read_csv(dir, index_col = False)

submission["y"].value_counts()