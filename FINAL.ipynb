{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mb7a0iJEGUEK",
        "6qaqs6LDHWEs",
        "u23Ox3iEI0Mp",
        "koHzQ4q1Jkim"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 인공지능 텀프로젝트 - 딥페이크 탐지 \n",
        "팀원: 정찬영, 정선영, 김태헌"
      ],
      "metadata": {
        "id": "qz8txBCiGH_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Image Selector"
      ],
      "metadata": {
        "id": "mb7a0iJEGUEK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV9FpQUoF7VX"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "\n",
        "fake_paths=glob('d:/AI_term_project/deepfake_1st/fake/*/*/*/*/*')\n",
        "real_paths=glob('d:/AI_term_project/deepfake_1st/real/*/*/*/*')\n",
        "\n",
        "print(len(fake_paths))\n",
        "print(len(real_paths))\n",
        "# 85177 \n",
        "# 13500"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "\n",
        "path_for_fake = 'd:/AI_term_project/select/fake/'  # fake image 저장할 폴더경로\n",
        "\n",
        "count = 0\n",
        "names = []\n",
        "for path in fake_paths:\n",
        "    images = glob(path+'/*.jpg')\n",
        "    names = []\n",
        "    while(len(names) <len(images) and len(names)<2): # choose 2 images per folder\n",
        "        random_choice = random.choice(images)\n",
        "        if(random_choice not in names):\n",
        "            shutil.copyfile(random_choice, path_for_fake+ str(count) +'.jpg')\n",
        "            count+=1\n",
        "            names.append(random_choice)"
      ],
      "metadata": {
        "id": "UGw21y5pGzvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_for_real = 'd:/AI_term_project/select/real/'  # real image 저장할 폴더경로\n",
        "\n",
        "count = 0\n",
        "names = []\n",
        "for path in real_paths:\n",
        "    images = glob(path+'/*.jpg')\n",
        "    names = []\n",
        "    while(len(names) <len(images) and len(names)<10): # choose 10 images per folder\n",
        "        random_choice = random.choice(images)\n",
        "        if(random_choice not in names):\n",
        "            shutil.copyfile(random_choice, path_for_real+ str(count) +'.jpg')\n",
        "            count+=1\n",
        "            names.append(random_choice)"
      ],
      "metadata": {
        "id": "TLlhvopuGD4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total fake image selected: 169868\n",
        "\n",
        "Total real image selected: 134967"
      ],
      "metadata": {
        "id": "OGl4uU_kHK1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Face Crop\n",
        "\n",
        "using MTCNN"
      ],
      "metadata": {
        "id": "6qaqs6LDHWEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mtcnn"
      ],
      "metadata": {
        "id": "zp0dM2LpGo7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "fake_image= glob('d:/AI_term_project/select/fake/*.jpg')\n",
        "fake_image.sort()\n",
        "print(len(fake_image))  # 169876\n",
        "real_image= glob('d:/AI_term_project/select/real/*.jpg')\n",
        "real_image.sort()\n",
        "print(len(real_image))  # 134967"
      ],
      "metadata": {
        "id": "pAXB5zlnHm3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MTCNN을 이용할 때 어쩌다 한번씩 매우 작은 영역을 얼굴로 인식하여 결과 리스트에 포함하기 때문에 \n",
        "# 만약 여러 개의 얼굴을 찾았다고 했을 시에 가장 큰 영역을 가지는 결과를 채택하는 함수\n",
        "def get_max_size_box(results):\n",
        "    max_idx = 0\n",
        "    max_size = 0\n",
        "    i = 0\n",
        "\n",
        "    for result in results:\n",
        "        box = result['box']\n",
        "        w = box[3]\n",
        "        h = box[2]\n",
        "        box_size = w * h\n",
        "        if max_size < box_size:\n",
        "            max_size = box_size\n",
        "            max_idx = i\n",
        "        i += 1\n",
        "    return max_idx"
      ],
      "metadata": {
        "id": "ce8lREP4HrC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "mIMFhVhTIYNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mtcnn import MTCNN\n",
        "import cv2\n",
        "err_path = []\n",
        "for i in range(, len(fake_image)): # face extract 할 path를 지정 (fake/real/test)\n",
        "    path = fake_image[i]  # face extract 할 path를 지정 (fake/real/test)\n",
        "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "    detector = MTCNN()\n",
        "    results = detector.detect_faces(img)\n",
        "\n",
        "  # mtcnn은 뒤집어진 얼굴은 인식을 못함. 실제 우리 데이터에도 뒤집어놓은 사진이 존재함.\n",
        "    if len(results)<1:\n",
        "        img = cv2.rotate(img, cv2.ROTATE_180)\n",
        "        results = detector.detect_faces(img)\n",
        "  \n",
        "  # 얼굴 못찾은경우\n",
        "    if len(results)<1:\n",
        "        print(path)\n",
        "        continue\n",
        "  \n",
        "    bounding_box = []\n",
        "    if len(results)==1:\n",
        "        bounding_box = results[0]['box']   # mtcnn의 detect_faces가 detect한 얼굴의 네 좌표\n",
        "    else:\n",
        "        idx = get_max_size_box(results)\n",
        "        bounding_box = results[idx]['box']\n",
        "  \n",
        "    margin_x = bounding_box[2] * 0.1  # 20% as margin\n",
        "    margin_y = bounding_box[3] * 0.135  # 27% as margin\n",
        "\n",
        "    x1 = int(bounding_box[0] - margin_x)\n",
        "    if x1 < 0:\n",
        "        x1 = 0\n",
        "        \n",
        "    x2 = int(bounding_box[0] + bounding_box[2] + margin_x)\n",
        "    if x2 > img.shape[1]:\n",
        "        x2 = img.shape[1]\n",
        "          \n",
        "    y1 = int(bounding_box[1] - margin_y)\n",
        "    if y1 < 0:\n",
        "        y1 = 0\n",
        "          \n",
        "    y2 = int(bounding_box[1] + bounding_box[3] + margin_y)\n",
        "    if y2 > img.shape[0]:\n",
        "        y2 = img.shape[0]\n",
        "\n",
        "    face_image = img[y1:y2,x1:x2].copy()\n",
        "\n",
        "    #face_image = cv2.resize(face_image, (380, 380)) \n",
        "\n",
        "    name = path.split('\\\\')[-1]\n",
        "    new_filename= 'd:/AI_term_project/face_crop/fake/' + name  # 여기도 fake/real/test 선택\n",
        "    cv2.imwrite(new_filename, cv2.cvtColor(face_image, cv2.COLOR_RGB2BGR))\n"
      ],
      "metadata": {
        "id": "54yWGY6nIc-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "real와 test에서는 전부 얼굴을 제대로 찾아서 crop 성공, fake에서는 변형을 매우 심하게 하여 얼굴처럼 보이지 않는 사진들이 존재. 그것들은 어차피 학습을 하여도 매우 쉬운 문제이기 때문에 그냥 무시."
      ],
      "metadata": {
        "id": "aOB8oSsmJGfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Preprocessing"
      ],
      "metadata": {
        "id": "u23Ox3iEI0Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "bE3wbKPtI-2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_paths=glob('d:/AI_term_project/face_crop/fake/*.jpg')\n",
        "real_paths=glob('d:/AI_term_project/face_crop/real/*.jpg')\n",
        "\n",
        "print(len(fake_paths))  # 169868\n",
        "print(len(real_paths))  # 134967\n",
        "\n",
        "all_paths=fake_paths+real_paths"
      ],
      "metadata": {
        "id": "BvVToPacJAdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Face Mask\n",
        "# 얼굴 일부를 가려서 학습을 진행.\n",
        "from imutils import face_utils\n",
        "import dlib\n",
        "detector = dlib.get_frontal_face_detector ()\n",
        "predictor = dlib.shape_predictor('d:/AI_term_project/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "def blind_face(img):\n",
        "    seed = random.random ()\n",
        "    probability = 0.5\n",
        "\n",
        "    if (seed < probability):\n",
        "        seed = random.random ()\n",
        "        case = int (seed * 3)\n",
        "        gray = cv2.cvtColor (img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        results = detector(gray, 0)\n",
        "        if len(results) > 0 :\n",
        "            shape = predictor (gray, results[0])\n",
        "            shape = face_utils.shape_to_np (shape)\n",
        "            pts_ = get_poly (shape, case, img.shape[1], img.shape[0])\n",
        "            cv2.fillConvexPoly (img, pts_, (0, 0, 0))\n",
        "            del shape, pts_\n",
        "\n",
        "        \n",
        "def get_poly(points, case_, x, y):\n",
        "    input_shape = (x, y)\n",
        "\n",
        "    pts = np.array ([points[36], points[20], points[23], points[45]], np.int32)\n",
        "\n",
        "    if case_ == 0:  # both eye\n",
        "        pts = np.array ([[points[36][0]-10, points[36][1]-20], [points[36][0]-10, points[36][1]+20], [points[45][0]+10, points[45][1]+20], [points[45][0]+10, points[45][1]-20]], np.int32)\n",
        "    elif case_ == 1:  # nose\n",
        "        pt1 = np.array ([points[31][0], points[28][1]])\n",
        "        pt2 = np.array ([points[35][0], points[28][1]])\n",
        "        pts = np.array ([points[27], pt1, points[31], points[51], points[35], pt2], np.int32)\n",
        "    elif case_ == 2:  # forehead\n",
        "        pt1 = np.array ([0, points[17][1]])\n",
        "        pt2 = np.array ([input_shape[0] - 1, points[26][1]])\n",
        "        pt3 = np.array ([input_shape[0] - 1, 0])\n",
        "        pt4 = np.array ([0, 0])\n",
        "        pts = np.array ([pt1, pt2, pt3, pt4], np.int32)\n",
        "    \n",
        "    return pts"
      ],
      "metadata": {
        "id": "S3eTlhFKJZC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "import albumentations as A\n",
        "\n",
        "labeling={'fake':1, 'real':0, 'test':2}\n",
        "\n",
        "def load_img_label(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    label = labeling[path.split('/')[-1][:4]]\n",
        "    return image, label\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, X, y, batch_size, step_per_epoch, input_shape, shuffle=True, augment = True):\n",
        "        self.X = X\n",
        "        self.y = y if y is not None else None\n",
        "        self.batch_size = batch_size\n",
        "        self.step_per_epoch = step_per_epoch\n",
        "        self.input_shape = input_shape\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.augment_size = int(self.batch_size*0.3)\n",
        "        self.init_generator()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def _shuffle_sample(self):\n",
        "        if (self.shuffle == True):\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def init_generator(self):\n",
        "        self.sample_size = len(self.X)\n",
        "        self.indexes = np.zeros(self.sample_size, dtype=np.int)\n",
        "\n",
        "        for i in range(self.sample_size):\n",
        "            self.indexes[i] = int(i)\n",
        "\n",
        "        self._shuffle_sample()\n",
        "\n",
        "        self.miniEpoch_size = self.batch_size * self.step_per_epoch\n",
        "        self.cnt_mini_epoch = self.sample_size // self.miniEpoch_size\n",
        "        self.iter_mini_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if (self.iter_mini_epoch < self.cnt_mini_epoch):\n",
        "            self.mini_indexes = self.indexes[self.iter_mini_epoch * self.miniEpoch_size: (self.iter_mini_epoch + 1) * self.miniEpoch_size]\n",
        "            self.iter_mini_epoch = self.iter_mini_epoch + 1\n",
        "        else:\n",
        "            self.iter_mini_epoch = 0\n",
        "            self._shuffle_sample()\n",
        "            self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.step_per_epoch)\n",
        "\n",
        "    def __data_generation(self, X_list, y_list):\n",
        "        X = np.zeros ((self.batch_size, self.input_shape[0], self.input_shape[1], self.input_shape[2]), dtype=np.float32)\n",
        "        y = np.zeros ((self.batch_size,), dtype=int)\n",
        "\n",
        "        if y_list is not None:\n",
        "            for i, (path, label) in enumerate(zip(X_list, y_list)):\n",
        "                img, label = load_img_label(path)\n",
        "                if self.augment and i < self.augment_size:\n",
        "                    img = augmentor(img)\n",
        "                X[i] = tf.image.resize(tf.cast(img, tf.float32), (224,224)) / 255.0  # resize the image to the same size\n",
        "                y[i] = label\n",
        "\n",
        "            return X, y\n",
        "        else:\n",
        "            for i, path in enumerate (X_list):\n",
        "                img, _ = load_img_label(path)\n",
        "                X[i] = tf.image.resize(tf.cast(img, tf.float32), (224,224)) / 255.0  # resize the image to the same size\n",
        "\n",
        "            return X\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.mini_indexes[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        X_list = [self.X[k] for k in indexes]\n",
        "\n",
        "        if self.y is not None:\n",
        "            y_list = [self.y[k] for k in indexes]\n",
        "            X, y = self.__data_generation (X_list, y_list)\n",
        "            return X, y\n",
        "        else:\n",
        "            y_list = None\n",
        "            X = self.__data_generation (X_list, y_list)\n",
        "            return X\n",
        "\n",
        "# Data Augmentation\n",
        "# face mask,\n",
        "# Horizontal flip,\n",
        "# GaussianBlur, GaussianNoise, ImageCompression\n",
        "def augmentor(img):\n",
        "    image = np.array(img)\n",
        "    blind_face(image)\n",
        "    transform = A.Compose([\n",
        "            A.augmentations.transforms.HorizontalFlip(p=0.5),\n",
        "            A.augmentations.transforms.GaussianBlur(p=0.3),\n",
        "            A.augmentations.transforms.GaussNoise(p=0.3),\n",
        "            A.augmentations.transforms.ImageCompression(quality_lower=60, quality_upper=100, p=0.3),\n",
        "    ])\n",
        "    transformed = transform(image=image)\n",
        "    image = transformed[\"image\"]\n",
        "    return image\n",
        "        "
      ],
      "metadata": {
        "id": "B7Y4C-dyJeEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(all_paths) = 304835, \n",
        "# 304835 / 6 = 50805\n",
        "train_data = DataGenerator(all_paths, all_paths, 6, 50805, (224, 224, 3), True, True)"
      ],
      "metadata": {
        "id": "m6STD0UFKTpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "사용할 모델인 EfficientNetB4의 연산에 이미 엄청나게 많은 메모리를 사용하기 때문에 batch_size를 크게 만들 수 없어 6으로 설정하였습니다. "
      ],
      "metadata": {
        "id": "aSp4Yfh4Kfer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model\n",
        "EfficientNetB4 + RectifiedAdam"
      ],
      "metadata": {
        "id": "koHzQ4q1Jkim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import efficientnet.tfkeras as efficientnet  # !pip install efficientnet\n",
        "import tensorflow_addons as tfa  # !pip install tensorflow-addons\n",
        "\n",
        "def model():\n",
        "    pretrained_model = efficientnet.EfficientNetB4(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(224, 224, 3),\n",
        "      )\n",
        "    pretrained_model.trainable = True\n",
        "  \n",
        "    inputs = keras.layers.Input(shape=(224, 224, 3))\n",
        "    x = pretrained_model(inputs)\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "    output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = keras.models.Model(inputs = inputs, outputs = output)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer = tfa.optimizers.RectifiedAdam(lr=1e-3, weight_decay=0.0005),  # 5 epoch 까지는 lr = 1e-3, 6epoch에서는 1e-4 사용\n",
        "        loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "        metrics = ['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "eVFdSPBhKuc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #model.summary() 출력결과\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " input_14 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
        "                                                                 \n",
        " efficientnet-b4 (Functional  (None, 7, 7, 1792)       17673816  \n",
        " )                                                               \n",
        "                                                                 \n",
        " global_average_pooling2d_6   (None, 1792)             0         \n",
        " (GlobalAveragePooling2D)                                        \n",
        "                                                                 \n",
        " dense_12 (Dense)            (None, 512)               918016    \n",
        "                                                                 \n",
        " dropout_5 (Dropout)         (None, 512)               0         \n",
        "                                                                 \n",
        " dense_13 (Dense)            (None, 1)                 513       \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 18,592,345\n",
        "Trainable params: 18,467,145\n",
        "Non-trainable params: 125,200\n",
        "_________________________________________________________________"
      ],
      "metadata": {
        "id": "lxkRVBZzLRGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import callbacks\n",
        "\n",
        "filepath = 'd:/AI_term_project/model2.hdf5'\n",
        "\n",
        "mcp = callbacks.ModelCheckpoint(filepath, monitor = 'loss', save_best_only=False, save_weights_only=True, mode='min', save_freq=50, verbose=0)"
      ],
      "metadata": {
        "id": "FoktGSYNLJV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data, epochs= 1, verbose=1, callbacks=[mcp])"
      ],
      "metadata": {
        "id": "ocEA_gA-LknC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "처음에 5epoch 학습한 후, 해당 weights에 lr 감소시켜서 1epoch 추가 학습"
      ],
      "metadata": {
        "id": "8VSUTEfaLnK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = 'd:/AI_term_project/test/test_crop/*.jpg'\n",
        "\n",
        "# collect all images\n",
        "images = glob(test_path)\n",
        "images.sort()\n",
        "print(len(images)) # 4100\n",
        "test_data = DataGenerator(images, None, 1, 4100, (224, 224, 3), False, False)"
      ],
      "metadata": {
        "id": "hfUNm0DwL3v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('d:/AI_term_project/rectifiedAdam_90.5%.hdf5')  # 5epoch 훈련한 weights, 이것만 했을 때는 90.5% accuracy\n",
        "p = model.predict(test_data)\n",
        "model.load_weights('d:/AI_term_project/89%.hdf5')  # 추가로 1epoch 훈련한 weights,이것만 했을 때는 89% accuracy\n",
        "p2 = model.predict(test_data)"
      ],
      "metadata": {
        "id": "eNMzrSo-L38y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = 'd:/AI_term_project/submission.csv'\n",
        "sc = open(dir, 'w')\n",
        "sc.write('path,y')\n",
        "sc.write('\\n')\n",
        "\n",
        "for i in range(len(images)):\n",
        "    if p[i]+p2[i] > 1.0:  # model ensemble\n",
        "        saveline = 'leaderboard/'+images[i][-15:] + ',1'\n",
        "        sc.write(saveline)\n",
        "        sc.write('\\n')\n",
        "    else:\n",
        "        saveline = 'leaderboard/'+images[i][-15:] + ',0'\n",
        "        sc.write(saveline)\n",
        "        sc.write('\\n')\n",
        "\n",
        "sc.close()\n",
        "\n",
        "submission = pd.read_csv(dir, index_col = False)\n",
        "print(submission[\"y\"].value_counts())\n",
        "\n",
        "# 출력결과\n",
        "# 0    2117\n",
        "# 1    1983\n",
        "# Name: y, dtype: int64"
      ],
      "metadata": {
        "id": "rMy1RTRBL9wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "최종 결과는 91.463%, public 리더보드 16위를 기록.\n",
        "\n",
        "이외에 모델 weight 갯수를 조금씩 바꾸거나 augmentation 비율조정 등을 하였지만 90% 넘어가는 모델은 만들지 못했고, 그나마 가장 높게 나왔던 두 개를 합쳐서 91%가 나왔다."
      ],
      "metadata": {
        "id": "B1jhpUIYMQPJ"
      }
    }
  ]
}